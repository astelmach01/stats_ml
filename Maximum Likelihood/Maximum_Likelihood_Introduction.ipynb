{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IMCwG1szSqNr"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDXj7V1RTu7k"
      },
      "source": [
        "# Understanding Your Dataset and Gaussian Distribution\n",
        "\n",
        "## Dataset Overview\n",
        "- **Observations**: You're working with a dataset that contains several observations of a single scalar variable (let's call it `x`). If you have `N` observations, your dataset might look something like this: `x_1, x_2, ..., x_N`.\n",
        "\n",
        "## Assumptions and Definitions\n",
        "- **Independence**: It's assumed that each observation in your dataset is drawn independently from every other one.\n",
        "- **Identically Distributed**: All observations come from the same Gaussian distribution, meaning they share the same mean (μ) and variance (σ²).\n",
        "- **I.I.D.**: This concept is so common that it has its own abbreviation: i.i.d., which stands for independent and identically distributed.\n",
        "\n",
        "## Gaussian Distribution\n",
        "- **Mean and Variance**: The Gaussian distribution is characterized by its mean (μ) and variance (σ²), but in this case, we don't know these values yet. They are the parameters we want to estimate from our dataset.\n",
        "\n",
        "## Joint Probability and Likelihood Function\n",
        "- **Product of Marginals**: Since the dataset is i.i.d., the joint probability of the dataset is the product of the marginal probabilities for each individual observation.\n",
        "- **Likelihood Function**: This joint probability is also known as the likelihood function when viewed as a function of the parameters (μ and σ²) given the data:\n",
        "\n",
        "$$\n",
        "{L}(\\mu, \\sigma^2 | \\mathbf{X}) = p(\\mathbf{X} | \\mu, \\sigma^2) = \\prod_{n=1}^N N(x_n | \\mu, \\sigma^2).\n",
        "$$\n",
        "\n",
        "Here, `N(x_n | μ, σ²)` represents the normal distribution for an individual observation, and the product over all `N` observations gives us the likelihood of the dataset given the parameters.\n",
        "\n",
        "## Objective\n",
        "- **Parameter Estimation**: Your goal is to find the values of μ and σ² that maximize the likelihood function, which would mean that the parameters are the most probable given the data observed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qj5b0cPW-Og"
      },
      "source": [
        "# Generating a Dataset from a Normal Distribution with PyTorch\n",
        "\n",
        "In this step, we're going to create a synthetic dataset by drawing samples from a normal (Gaussian) distribution. This is a common initial step in many statistical simulations and machine learning tasks, where you need a dataset to work with.\n",
        "\n",
        "We'll use the PyTorch library, which is a powerful tool for both deep learning and general scientific computing. Here's how we do it:\n",
        "\n",
        "1. **Define the Distribution Parameters**:\n",
        "   - The `mean` is the central tendency of the distribution.\n",
        "   - The `std` (standard deviation) measures the spread of the distribution.\n",
        "   - These are the (arbitrary) parameters we will estimate\n",
        "\n",
        "2. **Generate Samples**:\n",
        "   - We use the `torch.normal` function to draw samples from a normal distribution with the defined `mean` and `std`.\n",
        "   - The `size` argument in `torch.normal` specifies the number of samples to generate. Here, we ask for 100,000 samples.\n",
        "\n",
        "By executing this code, we create a `tensor` object in PyTorch that holds our dataset. This dataset can then be used for further analysis, such as parameter estimation or machine learning model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDgJpsYXW8_0",
        "outputId": "d8e294f0-1280-483b-e053-0bbaafccd759"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([11.4836,  9.6073,  8.9638,  ..., 14.5447,  9.0194,  8.9777])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean = 10.2\n",
        "std = 2.3\n",
        "\n",
        "# ask for 100,000 samples\n",
        "dataset = torch.normal(mean, std, size=(100_000,))\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_jAKu8xblic"
      },
      "source": [
        "# Why Use the Log Likelihood?\n",
        "\n",
        "In statistical modeling, especially when dealing with probability distributions, we often need to estimate parameters that best explain our data. The likelihood function, which is the probability of the data given the parameters, is central to this estimation process. However, when we work with the likelihood directly, we face the challenge of dealing with very small numbers because probabilities can be extremely small. This can lead to numerical underflow, where the computer rounds small numbers down to zero.\n",
        "\n",
        "To avoid this, we use the log likelihood. The logarithm is a monotonic transformation, meaning it preserves the order of values: if one value is larger than another, its logarithm is also larger. Therefore, maximizing the log likelihood is equivalent to maximizing the likelihood itself. Additionally, the logarithm has convenient mathematical properties that turn products into sums, making calculations easier and more numerically stable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k4h5IHGWiPZ"
      },
      "source": [
        "# Derivation of the Log Likelihood for a Normal Distribution\n",
        "\n",
        "The likelihood function for a set of observations assuming a normal distribution is the product of the probability density function (PDF) of each observation. Let's derive the log-likelihood from this starting point:\n",
        "\n",
        "## Step 1: The Normal Distribution PDF\n",
        "The PDF of a normally distributed random variable $x$ is given by:\n",
        "\n",
        "$$\n",
        "p(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right)\n",
        "$$\n",
        "\n",
        "## Step 2: The Likelihood Function\n",
        "For $N$ independent observations ${x_1, x_2, \\ldots, x_N}$, the likelihood function is the product of the individual PDFs:\n",
        "\n",
        "$$\n",
        "{L}(\\mu, \\sigma^2 | \\mathbf{x}) = \\prod_{n=1}^N p(x_n|\\mu, \\sigma^2)\n",
        "$$\n",
        "\n",
        "## Step 3: Log Likelihood\n",
        "Taking the natural logarithm of the likelihood function, we convert the product into a sum:\n",
        "\n",
        "$$\n",
        "\\ln {L}(\\mu, \\sigma^2 | \\mathbf{x}) = \\sum_{n=1}^N \\ln p(x_n|\\mu, \\sigma^2)\n",
        "$$\n",
        "\n",
        "## Step 4: Substituting the PDF\n",
        "Substitute the normal distribution PDF into the log likelihood:\n",
        "\n",
        "$$\n",
        "\\ln {L}(\\mu, \\sigma^2 | \\mathbf{x}) = \\sum_{n=1}^N \\ln \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{(x_n-\\mu)^2}{2\\sigma^2} \\right) \\right)\n",
        "$$\n",
        "\n",
        "## Step 5: Simplifying the Expression\n",
        "Using properties of logarithms, we simplify the equation:\n",
        "\n",
        "$$\n",
        "\\ln {L}(\\mu, \\sigma^2 | \\mathbf{x}) = \\sum_{n=1}^N \\left( -\\frac{(x_n-\\mu)^2}{2\\sigma^2} - \\ln \\sqrt{2\\pi\\sigma^2} \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\ln {L}(\\mu, \\sigma^2 | \\mathbf{x}) = -\\frac{1}{2\\sigma^2} \\sum_{n=1}^N (x_n-\\mu)^2 - \\frac{N}{2} \\ln (2\\pi\\sigma^2)\n",
        "$$\n",
        "\n",
        "Finally, we separate the constant terms from the variable terms:\n",
        "\n",
        "$$\n",
        "\\ln {L}(\\mu, \\sigma^2 | \\mathbf{x}) = -\\frac{1}{2\\sigma^2} \\sum_{n=1}^N (x_n-\\mu)^2 - \\frac{N}{2} \\ln \\sigma^2 - \\frac{N}{2} \\ln (2\\pi)\n",
        "$$\n",
        "\n",
        "This is the log likelihood function for the normal distribution that we use for maximum likelihood estimation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t_EHh-vcNmp"
      },
      "source": [
        "# Finding the Maximum Likelihood Estimate of the Mean\n",
        "\n",
        "Given the log likelihood function for a normal distribution:\n",
        "\n",
        "$$\n",
        "\\ln {L}(\\mu, \\sigma^2 | \\mathbf{x}) = -\\frac{1}{2\\sigma^2} \\sum_{n=1}^N (x_n - \\mu)^2 - \\frac{N}{2} \\ln \\sigma^2 - \\frac{N}{2} \\ln(2\\pi)\n",
        "$$\n",
        "\n",
        "We want to maximize this function with respect to $\\mu$\n",
        "\n",
        "## Derivative of the Log Likelihood Function\n",
        "\n",
        "The first step is to take the derivative of the log likelihood with respect to $\\mu$:\n",
        "\n",
        "$$\n",
        "\\frac{d}{d\\mu} \\ln {L}(\\mu, \\sigma^2 | \\mathbf{x}) = \\frac{d}{d\\mu} \\left( -\\frac{1}{2\\sigma^2} \\sum_{n=1}^N (x_n - \\mu)^2 \\right)\n",
        "$$\n",
        "\n",
        "Since $\\sigma^2$ is a constant, and the derivative of a sum is the sum of the derivatives, we can simplify this to:\n",
        "\n",
        "$$\n",
        "\\frac{d}{d\\mu} \\ln {L}(\\mu, \\sigma^2 | \\mathbf{x}) = -\\frac{1}{2\\sigma^2} \\sum_{n=1}^N \\frac{d}{d\\mu} (x_n - \\mu)^2\n",
        "$$\n",
        "\n",
        "The derivative of $(x_n - \\mu)^2$ with respect to $\\mu$ is $-2(x_n - \\mu)$. Plugging this back into the equation gives us:\n",
        "\n",
        "$$\n",
        "\\frac{d}{d\\mu} \\ln {L}(\\mu, \\sigma^2 | \\mathbf{x}) = \\frac{1}{\\sigma^2} \\sum_{n=1}^N (x_n - \\mu)\n",
        "$$\n",
        "\n",
        "## Setting the Derivative to Zero\n",
        "\n",
        "To find the maximum of the log likelihood, we set its derivative equal to zero:\n",
        "\n",
        "$$\n",
        "\\frac{1}{\\sigma^2} \\sum_{n=1}^N (x_n - \\mu) = 0\n",
        "$$\n",
        "\n",
        "Multiplying through by $\\sigma^2$ and then dividing by $N$ gives us:\n",
        "\n",
        "$$\n",
        "\\sum_{n=1}^N (x_n - \\mu) = 0\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sum_{n=1}^N x_n - N\\mu = 0\n",
        "$$\n",
        "\n",
        "Solving for $\\mu$ we get:\n",
        "\n",
        "$$\n",
        "\\mu = \\frac{1}{N} \\sum_{n=1}^N x_n\n",
        "$$\n",
        "\n",
        "This is the maximum likelihood estimate for the mean $ \\mu$, denoted as $\\mu_{ML}$, which is simply the average of all observations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0e6EAURdY7i"
      },
      "source": [
        "# Finding the Maximum Likelihood Estimate of the Variance\n",
        "\n",
        "After finding the maximum likelihood estimate for the mean $\\mu$, we proceed to estimate the variance $\\sigma^2$. We use the previously obtained $\\mu_{ML}$ in this process.\n",
        "\n",
        "## Starting with the Log Likelihood Function\n",
        "\n",
        "We have the log likelihood function given by:\n",
        "\n",
        "$$\n",
        "\\ln {L}(\\mu, \\sigma^2 | \\mathbf{x}) = -\\frac{1}{2\\sigma^2} \\sum_{n=1}^N (x_n - \\mu)^2 - \\frac{N}{2} \\ln \\sigma^2 - \\frac{N}{2} \\ln(2\\pi)\n",
        "$$\n",
        "\n",
        "## Derivative with Respect to $\\sigma^2$\n",
        "\n",
        "To maximize the log likelihood with respect to $\\sigma^2$, we take its derivative and set it to zero. The derivative of the log likelihood with respect to $\\sigma^2$ is:\n",
        "\n",
        "$$\n",
        "\\frac{d}{d\\sigma^2} \\ln {L}(\\mu, \\sigma^2 | \\mathbf{x}) = \\frac{d}{d\\sigma^2} \\left( -\\frac{1}{2\\sigma^2} \\sum_{n=1}^N (x_n - \\mu)^2 - \\frac{N}{2} \\ln \\sigma^2 \\right)\n",
        "$$\n",
        "\n",
        "Since the first term is the sum of $ -\\frac{1}{2\\sigma^2}$ times each squared difference, its derivative is the sum of $ \\frac{1}{2\\sigma^4}$ times each squared difference. The derivative of the second term $ -\\frac{N}{2} \\ln \\sigma^2 $ with respect to $ \\sigma^2 $ is $ -\\frac{N}{2\\sigma^2} $.\n",
        "\n",
        "Combining these gives us:\n",
        "\n",
        "$$\n",
        "\\frac{d}{d\\sigma^2} \\ln {L}(\\mu, \\sigma^2 | \\mathbf{x}) = \\frac{1}{2\\sigma^4} \\sum_{n=1}^N (x_n - \\mu)^2 - \\frac{N}{2\\sigma^2}\n",
        "$$\n",
        "\n",
        "## Setting the Derivative to Zero\n",
        "\n",
        "Now, we set the derivative to zero and solve for $ \\sigma^2$:\n",
        "\n",
        "$$\n",
        "\\frac{1}{2\\sigma^4} \\sum_{n=1}^N (x_n - \\mu)^2 - \\frac{N}{2\\sigma^2} = 0\n",
        "$$\n",
        "\n",
        "Multiplying through by $ 2\\sigma^4$ gives us:\n",
        "\n",
        "$$\n",
        "\\sum_{n=1}^N (x_n - \\mu)^2 = N\\sigma^2\n",
        "$$\n",
        "\n",
        "Dividing through by $N$ gives us the maximum likelihood estimate for the variance $ \\sigma^2$:\n",
        "\n",
        "$$\n",
        "\\sigma^2_{ML} = \\frac{1}{N} \\sum_{n=1}^N (x_n - \\mu_{ML})^2\n",
        "$$\n",
        "\n",
        "This is the sample variance, which is the mean of the squared differences between each observation and the sample mean $ \\mu_{ML}$. It's important to note that when using $ \\mu_{ML} $ for the mean in the variance formula, we're applying the maximum likelihood estimate for the mean that we derived earlier.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpNoxGPrc4Kx",
        "outputId": "85ec7cdd-f505-4eb6-dc78-d56867496d7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(10.2071), tensor(2.3099))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "estimated_mean = torch.mean(dataset)\n",
        "estimated_variance = torch.mean((dataset - estimated_mean) ** 2)\n",
        "\n",
        "estimated_mean, torch.sqrt(estimated_variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk6ohtz6gAAu"
      },
      "source": [
        "# Unbiased Estimation of Variance\n",
        "\n",
        "As we can see, the estimations are quite good. Play around with the sample size to see how this changes.\n",
        "While the maximum likelihood estimator for the mean of a Gaussian distribution is unbiased, the MLE for the variance is biased. This bias occurs because the MLE uses the sample mean, which itself is an estimator, and not the true population mean.\n",
        "\n",
        "## Expectation of MLE Variance\n",
        "\n",
        "The expectation of the maximum likelihood estimator of the variance is shown to be:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}[\\sigma^2_{ML}] = \\left( \\frac{N - 1}{N} \\right) \\sigma^2\n",
        "$$\n",
        "\n",
        "This indicates that the MLE for variance systematically underestimates the true variance.\n",
        "\n",
        "## Correction for Unbiased Variance\n",
        "\n",
        "To correct for this bias, we use the following unbiased estimator:\n",
        "\n",
        "$$\n",
        "\\hat{\\sigma}^2 = \\frac{N}{N - 1} \\sigma^2_{ML} = \\frac{1}{N - 1} \\sum_{n=1}^N (x_n - \\mu_{ML})^2\n",
        "$$\n",
        "\n",
        "This adjustment uses $ N - 1 $ in the denominator instead of $N$, which compensates for the bias introduced by the MLE.\n",
        "\n",
        "The code snippet provided demonstrates how to calculate this unbiased variance in Python using NumPy. By using `len(dataset) - 1`, we ensure that our variance estimate is unbiased and a more accurate reflection of the true population variance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46YFx36mhmE6",
        "outputId": "6e9c4b72-5b1d-47e1-93a2-5aa809ae62c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(2.3099), tensor(2.3099))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unbiased_variance = torch.sum((dataset - estimated_mean) ** 2) / (dataset.size(0) - 1)\n",
        "\n",
        "torch.sqrt(unbiased_variance), torch.sqrt(estimated_variance)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
